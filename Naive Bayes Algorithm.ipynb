{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recreate the Naive Bayes Algorithm\n",
    "## Steven Glover\n",
    "***\n",
    "\n",
    "To recreate the Naïve Bayes algorithm, I used a series of functions to compartmentalize the individual tasks and then created a function wrapper to execute the functions in sequence. The functions that the algorithm uses are as follows:\n",
    "\n",
    "**Probability Calculations:** \n",
    "1.\tdef independant_prob:  This function takes a column from a data frame in the form of an array. The function calculates the probability of each unique item in the array with respect to the entire array. It does this calculation in two stages. The first stage I used a combination of the numpy unique function and the zip function to create a dictionary where the keys are each unique value in the array and the values are the unique counts. The second stage was to create a dictionary of probabilities by dividing the dictionary of counts by the total number items in the original array. The function returns a dictionary of probabilities. \n",
    "\n",
    "**Preprocessing:**\n",
    "2.\tdef nb_preprocessing: This function takes care of several of the preprocessing tasks needed to generate predictions using the Naïve Bayes algorithm. The function takes two inputs: the dataset as a data frame and the name of the y variable. Prior to discussing the mechanics of this function, I would like to take a moment to discuss the strategy that I had to calculate the given likelihood component of the Naïve Bayes computation.  To achieve this, I subset the X_train dataset using the response variable and stored the two data frames to a python list.  I then used another custom function to generate a dictionary of the probabilities for each unique occurrence of a variable given the response (survived, died).  Additionally, I wanted to scale the function to effectively manage multiple y responses, which was achieved through the use of label encoding (more on this below). The function has four stages:\n",
    "    - Calculating the prior probability for the response, which was achieved by using the independant_prob() function discussed in ‘1’\n",
    "    - Label encoding the response -  label encoding converts a series of string variables to numerical. For instance, if the response was “died” or “lived” in the response, the label encoder would convert the strings to 0 and 1 values.  Why is this needed? Remember, the data frames that I plan to subset by the y response and store to a list? I will store each data frame in the list to the index that corresponds with the label encoded y variable. This allowed me to keep track of which subsets I used while processing the data iteratively. It also enables us to scale the project to work with more than a binary response. \n",
    "    - Create a test / train split\n",
    "    - Subset the X_train by the y response and store the data frames to a python list\n",
    "    - The function returns the test/train split with the list of X_train data frames, the names of the x variables, the label encoder object from sci-kit learn, and the prior probabilities in the form of a python dictionary.<br>\n",
    "\n",
    "<br>**Likelihood / Given Probabilities:**\n",
    "3.\tConditional_prob: This function returns a dictionary of dictionaries that contain the given probabilities or likelihoods for each class of a categorical variable given the response. The function takes two arguments: the list of X_train subsets and a list of the unique response variables, which were both returned using the nb_preprocessing described in #2. Additionally, the function utilized independant_prob that was described in #1 to generate the probability dictionary for each column of the data frames as it is accessed iteratively.<br>\n",
    "\n",
    "<br>**Generate Predictions:**<br>\n",
    "4.\tNaive_Bayes_Predictions: This function will return a list of the predictions as well as a list of the probabilities associated with the predictions. The function takes three input arguments: The X_test data frame, the dictionary of likelihoods, and the dictionary of prior probabilities.  Using the dictionary of likelihoods created in #3 and the dictionary of prior probabilities created in #1, this function iteratively calculates the posterior probability of each y response for each line of the X_test training set. It then chooses the y response with the greater probability for the line and appends the prediction and the probability to python lists with each iteration.  After the function has completed this process for every line of the test_X dataset both the lists are returned. <br>\n",
    "\n",
    "<br>**Prediction Accuracy Report:**<br>\n",
    "5.\tThe function returns the accuracy score, precision/recall tables, and a confusion matrix for the predictions. \n",
    "\n",
    "**The Wrapper Function:**\n",
    "6.\tComplete_NaiveBayes: The function wraps all the above functions together to provide preprocessing, generate predictions using the Naïve Bayes algorithm, print an accuracy report with the label encoding mappings, and return the lists of predictions/probabilities. The function can do all of this with only two inputs: the data frame and the name of the y response. \n",
    "\n",
    "**<font color = red> Note: </font>** the function requires that the datatypes for all columns be objects /strings. You will receive an error if you try to feed the function a data frame that has columns with a float or int datatypes. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import and Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns:  Index(['pclass', 'survived', 'name', 'sex', 'age', 'sibsp', 'parch', 'ticket',\n",
      "       'fare', 'cabin', 'embarked', 'boat', 'body', 'home.dest'],\n",
      "      dtype='object') \n",
      "\n",
      "shape:  (1310, 14) \n",
      "\n",
      "null: \n",
      " pclass          1\n",
      "survived        1\n",
      "name            1\n",
      "sex             1\n",
      "age           264\n",
      "sibsp           1\n",
      "parch           1\n",
      "ticket          1\n",
      "fare            2\n",
      "cabin        1015\n",
      "embarked        3\n",
      "boat          824\n",
      "body         1189\n",
      "home.dest     565\n",
      "dtype: int64\n",
      "-------------------------------\n",
      "Unique Values:  pclass          3\n",
      "survived        2\n",
      "name         1307\n",
      "sex             2\n",
      "age            98\n",
      "sibsp           7\n",
      "parch           8\n",
      "ticket        929\n",
      "fare          281\n",
      "cabin         186\n",
      "embarked        3\n",
      "boat           27\n",
      "body          121\n",
      "home.dest     369\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('C:/Users/Steven Glover/Jupyter Notebooks/Fall Semester/KNN/titanic3.csv')\n",
    "print('columns: ',data.columns,'\\n')\n",
    "print('shape: ', data.shape,'\\n')\n",
    "print('null: \\n', pd.isnull(data).sum())\n",
    "print('-------------------------------')\n",
    "print('Unique Values: ', data.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Name, Cabin, Ticket, Boat & Get Rid of Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1043 entries, 0 to 1042\n",
      "Data columns (total 8 columns):\n",
      "pclass      1043 non-null float64\n",
      "survived    1043 non-null float64\n",
      "sex         1043 non-null object\n",
      "age         1043 non-null float64\n",
      "sibsp       1043 non-null float64\n",
      "parch       1043 non-null float64\n",
      "fare        1043 non-null float64\n",
      "embarked    1043 non-null object\n",
      "dtypes: float64(6), object(2)\n",
      "memory usage: 65.3+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pclass \n",
      " [ 1.  2.  3.]\n",
      "---------------\n",
      "survived \n",
      " [ 1.  0.]\n",
      "---------------\n",
      "sex \n",
      " ['female' 'male']\n",
      "---------------\n",
      "age \n",
      " [ 29.       0.9167   2.      30.      25.      48.      63.      39.      53.\n",
      "  71.      47.      18.      24.      26.      80.      50.      32.      36.\n",
      "  37.      42.      19.      35.      28.      45.      40.      58.      22.\n",
      "  41.      44.      59.      60.      33.      17.      11.      14.      49.\n",
      "  76.      46.      27.      64.      55.      70.      38.      51.      31.\n",
      "   4.      54.      23.      43.      52.      16.      32.5     21.      15.\n",
      "  65.      28.5     45.5     56.      13.      61.      34.       6.      57.\n",
      "  62.      67.       1.      12.      20.       0.8333   8.       0.6667\n",
      "   7.       3.      36.5     18.5      5.      66.       9.       0.75\n",
      "  70.5     22.5      0.3333   0.1667  40.5     10.      23.5     34.5     20.5\n",
      "  30.5     55.5     38.5     14.5     24.5     74.       0.4167  11.5     26.5   ]\n",
      "---------------\n",
      "sibsp \n",
      " [ 0.  1.  2.  3.  4.  5.  8.]\n",
      "---------------\n",
      "parch \n",
      " [ 0.  2.  1.  4.  3.  5.  6.]\n",
      "---------------\n",
      "fare \n",
      " [ 211.3375  151.55     26.55     77.9583    0.       51.4792   49.5042\n",
      "  227.525    69.3      78.85     30.      247.5208   76.2917   75.2417\n",
      "   52.5542  221.7792   26.       91.0792  135.6333   35.5      31.\n",
      "  164.8667  262.375    55.       30.5      50.4958   27.7208  134.5\n",
      "   26.2875   27.4458  512.3292    5.       47.1     120.       61.175\n",
      "   53.1      86.5      29.7     136.7792   25.5875   83.1583   25.7      71.\n",
      "   71.2833   52.       57.       81.8583  106.425    39.6      56.9292\n",
      "   78.2667   31.6792   26.3875   27.75    263.      133.65     49.5      79.2\n",
      "   38.5     211.5      59.4      89.1042   34.6542   28.5     153.4625\n",
      "   63.3583   55.4417   76.7292   83.475    93.5      42.5      50.\n",
      "   57.9792   90.       28.7125   51.8625   25.9292   39.4      45.5\n",
      "  146.5208   82.1708   57.75     42.4     113.275    26.2833  108.9\n",
      "   61.9792   66.6      40.125    55.9      30.6958   60.       82.2667\n",
      "   32.3208   79.65    110.8833   28.5375   33.5      34.0208   75.25\n",
      "   77.2875   61.3792   24.       13.       11.5      10.5      12.525    39.\n",
      "   29.       21.       13.5      26.25     36.75     73.5      31.5      23.\n",
      "   32.5      13.8583   14.5      33.       65.       16.       12.275    27.\n",
      "   15.       13.7917   12.35     41.5792   12.       12.875    37.0042\n",
      "   19.5      14.        9.6875   30.0708   13.8625   12.7375   15.0333\n",
      "   15.0458   18.75     12.65     15.75      7.55     20.25      7.65\n",
      "    7.925     7.2292    7.25      8.05      9.475     9.35     18.7875\n",
      "    7.8875    7.05      8.3      22.525     7.8542   31.275     7.775\n",
      "    7.7958    7.8958   17.8      31.3875    7.225    14.4583   15.85\n",
      "   19.2583   14.4542    7.8792    4.0125   56.4958    7.75     15.2458\n",
      "   15.5      16.1       7.725     7.0458    7.2833    7.8208    6.75\n",
      "    8.6625    7.7333    7.4958    7.6292   15.9       8.1583   10.5167\n",
      "   10.1708    6.95     14.4      24.15     17.4       9.5      20.575\n",
      "   12.475    13.9       6.975    15.1      34.375     7.7417   20.525\n",
      "    7.85     46.9       8.3625    9.8458    8.85     14.1083    8.9625\n",
      "   12.2875    6.45      7.0542    6.4958    8.6542   11.1333    9.825\n",
      "    7.125     8.4333    7.5208   13.4167    7.8292   22.025    12.1833\n",
      "    9.5875    9.4833    6.4375   15.55      7.5792    7.1417    8.0292\n",
      "   15.7417   11.2417    7.8       6.2375    9.225     3.1708    8.4042\n",
      "    9.2167    8.6833   21.075    39.6875   13.775    29.125    20.2125\n",
      "   69.55      9.325    16.7      27.9       9.8375   10.4625    8.5167\n",
      "    9.8417    9.       18.        7.        7.875 ]\n",
      "---------------\n",
      "embarked \n",
      " ['S' 'C' 'Q']\n",
      "---------------\n"
     ]
    }
   ],
   "source": [
    "data = data.drop(['name','cabin','ticket','boat','home.dest','body'], axis = 1)\n",
    "data = data.dropna()\n",
    "data = data.reset_index(drop = True)\n",
    "display(data.info())\n",
    "\n",
    "for i in data.columns.tolist():\n",
    "    print(i,'\\n',data[i].unique())\n",
    "    print('---------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert All data columns to a string prior to preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = data[['pclass' , 'sex' , 'embarked' , 'survived']]\n",
    "\n",
    "for col in data.columns.tolist():\n",
    "    data[col] = data[col].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color = blue> The following functions and code is the prototyping for the Complete_NB function in the .py script file </font>\n",
    "The NB script file will be called at the end of the file to ensure that it works correctly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def independant_prob(arr, show = 'n'):\n",
    "    '''This function will return a dictionary of the unique values in an array and counts\n",
    "    for that value in the form of a python dictionary. It will also determine the independant probability\n",
    "    and return as dictionary'''\n",
    "    #get the count dictionary\n",
    "    cat_dict = dict(zip(np.unique(arr, return_counts= True)[0],np.unique(arr, return_counts= True)[1]))\n",
    "    prob_dict = {}\n",
    "    total = sum(cat_dict.values())\n",
    "    for key in cat_dict.keys():\n",
    "        prob_dict[key] = cat_dict[key] / total\n",
    "    \n",
    "    #Return a Summary of the calcations\n",
    "    if show == 'y':\n",
    "        print('Count Dictionary:')\n",
    "        print(cat_dict,'\\n')\n",
    "        print('Probability Dictionary')\n",
    "        print(prob_dict)\n",
    "    \n",
    "    return prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_prob(data_list, uniqueY):\n",
    "    given_prob_dict = {}\n",
    "    for data_set in range(len(uniqueY)):\n",
    "        df = data_list[data_set]\n",
    "        y_given = {}\n",
    "        for cat in df.columns.tolist():\n",
    "        #need to make sure that the input stays strings at conversion\n",
    "            y_given[str(cat)] = independant_prob(np.array(df[cat]).astype(str))\n",
    "        #need to keep the float string format\n",
    "        given_prob_dict[str(float(data_set))] = y_given \n",
    "    return given_prob_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### Subset X_train based off output categories and each to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_preprocessing(df, y):\n",
    "    from sklearn.cross_validation import train_test_split\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    #subset X and Y\n",
    "    x_names = [x for x in df.columns.tolist() if x not in [y]]\n",
    "    X = df[x_names]\n",
    "    \n",
    "    #store the y column name and identify the y variable\n",
    "    y_col = y\n",
    "    y = df[y_col]\n",
    "\n",
    "    \"\"\"In order for this fuction to be able to used with multiple classes\n",
    "    the y values should be label encoded. The label encoding is necessary\n",
    "    because when the naive bayes algorithm is iteratively making its predictions,\n",
    "    it  will need to associate the prediction with the position in the list its X subset\n",
    "    is in. To easily calculate given probabilites, I will subset the X_train by its Y_train category.\"\"\"\n",
    "    \n",
    "    #label encode the y \n",
    "    \"\"\"note the encoded y will still needed to be converted back to a string for the train test split.\n",
    "    I will need to account for this when I split X_train into subsets.\"\"\"\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    \n",
    "    y = y.astype('float').astype('str')\n",
    "    \n",
    "    y_prob = independant_prob(y.astype(str))\n",
    "    \n",
    "    # Test Train Split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    # Turn the datasets back into dataframes\n",
    "    X_train = pd.DataFrame(data = X_train, columns = x_names).reset_index(drop=True)\n",
    "    X_test = pd.DataFrame(data = X_test, columns = x_names).reset_index(drop=True)\n",
    "    y_train = pd.DataFrame(data = y_train, columns = ['survived']).reset_index(drop=True)\n",
    "    y_test = pd.DataFrame(data = y_test, columns = ['survived']).reset_index(drop=True)\n",
    "\n",
    "    #--------------------------------------\n",
    "    #sort unique label encoded Ys. \n",
    "    \"\"\"since the Ys have been label encoded and sorted, the X_train\n",
    "    dataframes will be stored to the list in the order assosciated with the y\n",
    "    variable. This will allow us to correspond the y responce with the position in the list\n",
    "    when generating the predictions.\"\"\"\n",
    "    \n",
    "    uniqueY = sorted(pd.Series(y).astype(float).unique().tolist())\n",
    "    X_train_list = []\n",
    "    \n",
    "    for subset in range(len(uniqueY)):\n",
    "        subset_df = X_train[y_train.survived == str(float(subset))]\n",
    "        # Make sure every column is still a string \n",
    "        for col in subset_df.columns.tolist():\n",
    "            subset_df[col] == subset_df[col].astype(str)\n",
    "        # add subset to a list\n",
    "        X_train_list.append(subset_df)\n",
    "                        \n",
    "    return X_train_list, X_test, y_train, y_test, uniqueY, x_names, le, y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[['pclass' , 'sex' , 'embarked' , 'survived']] \n",
    "X_train_list, X_test, y_train, y_test, uniqueY, x_names, label_encoder, y_prob = nb_preprocessing(data, 'survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes Calculation\n",
    "Each piece will be broken down into a different function and combined. We will use the pclass to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived Probabilites\n",
      "----------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0.0': 0.59252157238734415, '1.0': 0.40747842761265579}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Survived Probabilites')\n",
    "print('----------------------------')\n",
    "survived_prob = independant_prob(np.array(data.survived).astype(str))\n",
    "survived_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.0': {'embarked': {'C': 0.13934426229508196,\n",
       "   'Q': 0.061475409836065573,\n",
       "   'S': 0.79918032786885251},\n",
       "  'pclass': {'1.0': 0.17008196721311475,\n",
       "   '2.0': 0.23360655737704919,\n",
       "   '3.0': 0.59631147540983609},\n",
       "  'sex': {'female': 0.17418032786885246, 'male': 0.82581967213114749}},\n",
       " '1.0': {'embarked': {'C': 0.31502890173410403,\n",
       "   'Q': 0.034682080924855488,\n",
       "   'S': 0.6502890173410405},\n",
       "  'pclass': {'1.0': 0.42196531791907516,\n",
       "   '2.0': 0.2861271676300578,\n",
       "   '3.0': 0.29190751445086704},\n",
       "  'sex': {'female': 0.69075144508670516, 'male': 0.30924855491329478}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probabilities = conditional_prob(X_train_list, uniqueY)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Predictions and Probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes_Predictions(X_test,probabilities, survived_prob):\n",
    "    testObs = X_test\n",
    "    classification_list = []\n",
    "    classification_probs = []\n",
    "    #iterate over each row\n",
    "    for row in range(testObs.shape[0]):\n",
    "        predictions = {}\n",
    "        #calcuate the probablity for each outcome seperately\n",
    "        for outcome in survived_prob.keys():\n",
    "            outcome = str(outcome)\n",
    "            #iterate over each column in the row\n",
    "            prob_list = []\n",
    "            prob_list.append(survived_prob[outcome])\n",
    "            for col in testObs.columns.tolist():\n",
    "                #pull probabilites out of 3x nested dictionary. keys are as follows: y response, column, \n",
    "                #category in the column\n",
    "                prob_list.append(probabilities[outcome][col][testObs.loc[row, col]])   \n",
    "            predictions[outcome] = np.prod(np.array(prob_list)) \n",
    "        \n",
    "        #get the prediction of the greatest number\n",
    "        pred_key = max(predictions, key=predictions.get)\n",
    "        classification_list.append(pred_key)\n",
    "        classification_probs.append(predictions[pred_key] / sum(predictions.values()))\n",
    "        \n",
    "    return classification_list, classification_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_list, classification_probs = Naive_Bayes_Predictions(X_test, probabilities, survived_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuaracy_report(y_test, predictions):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    print('The Accuracy Score is: ',np.array(np.sum(np.equal(predictions, y_test)) / y_test.shape[0]),'\\n')\n",
    "    print('The Classification Report')\n",
    "    print('-------------------------')\n",
    "    print(classification_report(y_test, predictions),'\\n')\n",
    "    print('The Confusion Matrix')\n",
    "    print('-------------------------')\n",
    "    print(pd.DataFrame(confusion_matrix(y_test, predictions)).apply(lambda x: x / sum(x), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score is:  0.8229665071770335 \n",
      "\n",
      "The Classification Report\n",
      "-------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.84      0.88      0.86       130\n",
      "        1.0       0.78      0.73      0.76        79\n",
      "\n",
      "avg / total       0.82      0.82      0.82       209\n",
      " \n",
      "\n",
      "The Confusion Matrix\n",
      "-------------------------\n",
      "          0         1\n",
      "0  0.876923  0.123077\n",
      "1  0.265823  0.734177\n"
     ]
    }
   ],
   "source": [
    "accuaracy_report(np.array(y_test),np.array(classification_list).reshape(len(classification_list),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring it all together prototype for the .py script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_NaiveBayes_test(df, y):\n",
    "    #check for categorical variables: must be an object or a category\n",
    "    for col in data.columns.tolist():\n",
    "        if data[col].dtype != 'object':\n",
    "            return 'All columns must have a dytpe of object'\n",
    "        \n",
    "    # get test train list\n",
    "    X_train_list, X_test, y_train, y_test, uniqueY, x_names, label_encoder, y_prob  = nb_preprocessing(df, 'survived')\n",
    "        \n",
    "    # generate conditional probabilites     \n",
    "    probabilities = conditional_prob(X_train_list, uniqueY)\n",
    "    \n",
    "    # generate predictions and the list of probabilites\n",
    "    classification_list, classification_probs = Naive_Bayes_Predictions(X_test,probabilities, y_prob)\n",
    "    \n",
    "    # print the accuracy report\n",
    "    accuaracy_report(np.array(y_test),np.array(classification_list).reshape(len(classification_list),1))\n",
    "    \n",
    "    #return the label encoding categories\n",
    "    print('\\nThe label encoding mappings')\n",
    "    print('-----------------------------')\n",
    "    for val in uniqueY:\n",
    "        print(label_encoder.inverse_transform(int(val)),' : ',val)\n",
    "        \n",
    "    #return the classification list and the probabilities    \n",
    "    print('\\nThe Classification Report & Classification Probabilites')\n",
    "    print('---------------------------------------------------------')       \n",
    "    return classification_list, classification_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score is:  0.8038277511961722 \n",
      "\n",
      "The Classification Report\n",
      "-------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.80      0.83      0.82       109\n",
      "        1.0       0.81      0.77      0.79       100\n",
      "\n",
      "avg / total       0.80      0.80      0.80       209\n",
      " \n",
      "\n",
      "The Confusion Matrix\n",
      "-------------------------\n",
      "          0         1\n",
      "0  0.834862  0.165138\n",
      "1  0.230000  0.770000\n",
      "\n",
      "The label encoding mappings\n",
      "-----------------------------\n",
      "0.0  :  0.0\n",
      "1.0  :  1.0\n",
      "\n",
      "The Classification Report & Classification Probabilites\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0'],\n",
       " [0.78775989430241911,\n",
       "  0.94180825484445962,\n",
       "  0.84948316289425108,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.84948316289425108,\n",
       "  0.84948316289425108,\n",
       "  0.94180825484445962,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.73968076719323239,\n",
       "  0.89069062948865463,\n",
       "  0.94180825484445962,\n",
       "  0.84948316289425108,\n",
       "  0.7302698684771578,\n",
       "  0.61694253316035208,\n",
       "  0.84948316289425108,\n",
       "  0.56413723397226689,\n",
       "  0.94014003290760462,\n",
       "  0.7302698684771578,\n",
       "  0.94180825484445962,\n",
       "  0.89069062948865463,\n",
       "  0.84948316289425108,\n",
       "  0.55222193655363205,\n",
       "  0.94180825484445962,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.61694253316035208,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.55222193655363205,\n",
       "  0.78775989430241911,\n",
       "  0.84948316289425108,\n",
       "  0.89069062948865463,\n",
       "  0.61694253316035208,\n",
       "  0.94180825484445962,\n",
       "  0.84948316289425108,\n",
       "  0.55222193655363205,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.94014003290760462,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.94180825484445962,\n",
       "  0.7302698684771578,\n",
       "  0.89069062948865463,\n",
       "  0.60981949718508255,\n",
       "  0.94180825484445962,\n",
       "  0.89069062948865463,\n",
       "  0.73968076719323239,\n",
       "  0.89069062948865463,\n",
       "  0.64035704749648759,\n",
       "  0.94180825484445962,\n",
       "  0.94180825484445962,\n",
       "  0.89069062948865463,\n",
       "  0.61694253316035208,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.55222193655363205,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.55222193655363205,\n",
       "  0.84948316289425108,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.55222193655363205,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.77956914642975661,\n",
       "  0.89069062948865463,\n",
       "  0.94180825484445962,\n",
       "  0.73968076719323239,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.94180825484445962,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.77956914642975661,\n",
       "  0.89069062948865463,\n",
       "  0.61694253316035208,\n",
       "  0.55222193655363205,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.55222193655363205,\n",
       "  0.89069062948865463,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.77956914642975661,\n",
       "  0.7302698684771578,\n",
       "  0.7302698684771578,\n",
       "  0.7302698684771578,\n",
       "  0.73968076719323239,\n",
       "  0.89069062948865463,\n",
       "  0.60981949718508255,\n",
       "  0.89069062948865463,\n",
       "  0.84948316289425108,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.60981949718508255,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.64035704749648759,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.84948316289425108,\n",
       "  0.94180825484445962,\n",
       "  0.60981949718508255,\n",
       "  0.88589680652461644,\n",
       "  0.94180825484445962,\n",
       "  0.61694253316035208,\n",
       "  0.89069062948865463,\n",
       "  0.55222193655363205,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.56413723397226689,\n",
       "  0.64035704749648759,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.84948316289425108,\n",
       "  0.55222193655363205,\n",
       "  0.64035704749648759,\n",
       "  0.94180825484445962,\n",
       "  0.94180825484445962,\n",
       "  0.55222193655363205,\n",
       "  0.78775989430241911,\n",
       "  0.55222193655363205,\n",
       "  0.77956914642975661,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.73968076719323239,\n",
       "  0.55222193655363205,\n",
       "  0.7302698684771578,\n",
       "  0.55222193655363205,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.84948316289425108,\n",
       "  0.7302698684771578,\n",
       "  0.94180825484445962,\n",
       "  0.7302698684771578,\n",
       "  0.61694253316035208,\n",
       "  0.77956914642975661,\n",
       "  0.89069062948865463,\n",
       "  0.89069062948865463,\n",
       "  0.94180825484445962,\n",
       "  0.84948316289425108,\n",
       "  0.61694253316035208,\n",
       "  0.94180825484445962,\n",
       "  0.78775989430241911,\n",
       "  0.78775989430241911,\n",
       "  0.89069062948865463,\n",
       "  0.7302698684771578,\n",
       "  0.78775989430241911,\n",
       "  0.7302698684771578,\n",
       "  0.89069062948865463,\n",
       "  0.78775989430241911,\n",
       "  0.7302698684771578,\n",
       "  0.7302698684771578,\n",
       "  0.7302698684771578,\n",
       "  0.64035704749648759,\n",
       "  0.84948316289425108,\n",
       "  0.94180825484445962,\n",
       "  0.78775989430241911,\n",
       "  0.55222193655363205,\n",
       "  0.84948316289425108,\n",
       "  0.64035704749648759,\n",
       "  0.60981949718508255,\n",
       "  0.7302698684771578,\n",
       "  0.60981949718508255,\n",
       "  0.89069062948865463,\n",
       "  0.55222193655363205,\n",
       "  0.77956914642975661,\n",
       "  0.61694253316035208,\n",
       "  0.73968076719323239,\n",
       "  0.55222193655363205,\n",
       "  0.55222193655363205,\n",
       "  0.64035704749648759])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Complete_NaiveBayes_test(data, 'survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy Score is:  0.7942583732057417 \n",
      "\n",
      "The Classification Report\n",
      "-------------------------\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.81      0.82      0.81       115\n",
      "        1.0       0.77      0.77      0.77        94\n",
      "\n",
      "avg / total       0.79      0.79      0.79       209\n",
      " \n",
      "\n",
      "The Confusion Matrix\n",
      "-------------------------\n",
      "          0         1\n",
      "0  0.817391  0.182609\n",
      "1  0.234043  0.765957\n",
      "\n",
      "The label encoding mappings\n",
      "-----------------------------\n",
      "0.0  :  0.0\n",
      "1.0  :  1.0\n",
      "\n",
      "The Classification Report & Classification Probabilites\n",
      "---------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0',\n",
       "  '0.0',\n",
       "  '1.0',\n",
       "  '0.0'],\n",
       " [0.5666041581664083,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.56556319712432712,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.9428132546708935,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.89155474622576247,\n",
       "  0.74862123856812102,\n",
       "  0.72649261979479773,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.85657099938511494,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.59444749994407386,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.93329879366876045,\n",
       "  0.64282203137284899,\n",
       "  0.8018394889512056,\n",
       "  0.5666041581664083,\n",
       "  0.605351856852542,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.64282203137284899,\n",
       "  0.72649261979479773,\n",
       "  0.5666041581664083,\n",
       "  0.74862123856812102,\n",
       "  0.8018394889512056,\n",
       "  0.64282203137284899,\n",
       "  0.5666041581664083,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.59444749994407386,\n",
       "  0.74862123856812102,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.85657099938511494,\n",
       "  0.89155474622576247,\n",
       "  0.60947804337989719,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.605351856852542,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.9428132546708935,\n",
       "  0.9428132546708935,\n",
       "  0.5666041581664083,\n",
       "  0.87999140089391914,\n",
       "  0.9428132546708935,\n",
       "  0.74862123856812102,\n",
       "  0.9428132546708935,\n",
       "  0.5666041581664083,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.9428132546708935,\n",
       "  0.605351856852542,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.78303790588091959,\n",
       "  0.8018394889512056,\n",
       "  0.8018394889512056,\n",
       "  0.9428132546708935,\n",
       "  0.8018394889512056,\n",
       "  0.89155474622576247,\n",
       "  0.9428132546708935,\n",
       "  0.85657099938511494,\n",
       "  0.5666041581664083,\n",
       "  0.89155474622576247,\n",
       "  0.93329879366876045,\n",
       "  0.605351856852542,\n",
       "  0.8018394889512056,\n",
       "  0.85657099938511494,\n",
       "  0.8018394889512056,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.9428132546708935,\n",
       "  0.8018394889512056,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.72649261979479773,\n",
       "  0.5666041581664083,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.85657099938511494,\n",
       "  0.85657099938511494,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.85657099938511494,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.605351856852542,\n",
       "  0.64282203137284899,\n",
       "  0.8018394889512056,\n",
       "  0.85657099938511494,\n",
       "  0.8018394889512056,\n",
       "  0.72649261979479773,\n",
       "  0.93329879366876045,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.8018394889512056,\n",
       "  0.72649261979479773,\n",
       "  0.89155474622576247,\n",
       "  0.64282203137284899,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.64282203137284899,\n",
       "  0.85657099938511494,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.605351856852542,\n",
       "  0.5666041581664083,\n",
       "  0.5666041581664083,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.72649261979479773,\n",
       "  0.64282203137284899,\n",
       "  0.56556319712432712,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.89155474622576247,\n",
       "  0.85657099938511494,\n",
       "  0.8018394889512056,\n",
       "  0.5666041581664083,\n",
       "  0.8018394889512056,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.74862123856812102,\n",
       "  0.64282203137284899,\n",
       "  0.64282203137284899,\n",
       "  0.93329879366876045,\n",
       "  0.9428132546708935,\n",
       "  0.85657099938511494,\n",
       "  0.64282203137284899,\n",
       "  0.8018394889512056,\n",
       "  0.56556319712432712,\n",
       "  0.89155474622576247,\n",
       "  0.89155474622576247,\n",
       "  0.85657099938511494,\n",
       "  0.72649261979479773,\n",
       "  0.87999140089391914,\n",
       "  0.89155474622576247,\n",
       "  0.9428132546708935,\n",
       "  0.9428132546708935,\n",
       "  0.89155474622576247,\n",
       "  0.59444749994407386,\n",
       "  0.605351856852542,\n",
       "  0.8018394889512056,\n",
       "  0.9428132546708935,\n",
       "  0.5666041581664083,\n",
       "  0.74862123856812102,\n",
       "  0.64282203137284899,\n",
       "  0.605351856852542,\n",
       "  0.9428132546708935,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.8018394889512056,\n",
       "  0.5666041581664083,\n",
       "  0.72649261979479773,\n",
       "  0.78303790588091959,\n",
       "  0.85657099938511494,\n",
       "  0.605351856852542,\n",
       "  0.89155474622576247,\n",
       "  0.64282203137284899,\n",
       "  0.74862123856812102,\n",
       "  0.64282203137284899,\n",
       "  0.64282203137284899,\n",
       "  0.9428132546708935,\n",
       "  0.56556319712432712,\n",
       "  0.89155474622576247,\n",
       "  0.5666041581664083,\n",
       "  0.64282203137284899,\n",
       "  0.89155474622576247,\n",
       "  0.72649261979479773,\n",
       "  0.8018394889512056])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from NaiveBayes import Complete_NaiveBayes\n",
    "Complete_NaiveBayes(data, 'survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I converted the response from 0 to 1 to died, survived to illustrate the functionality of the label encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       survived\n",
       "1       survived\n",
       "2           died\n",
       "3           died\n",
       "4           died\n",
       "5       survived\n",
       "6       survived\n",
       "7           died\n",
       "8       survived\n",
       "9           died\n",
       "10          died\n",
       "11      survived\n",
       "12      survived\n",
       "13      survived\n",
       "14      survived\n",
       "15          died\n",
       "16      survived\n",
       "17      survived\n",
       "18          died\n",
       "19      survived\n",
       "20      survived\n",
       "21      survived\n",
       "22      survived\n",
       "23      survived\n",
       "24          died\n",
       "25      survived\n",
       "26      survived\n",
       "27      survived\n",
       "28      survived\n",
       "29          died\n",
       "          ...   \n",
       "1013        died\n",
       "1014        died\n",
       "1015        died\n",
       "1016        died\n",
       "1017        died\n",
       "1018        died\n",
       "1019        died\n",
       "1020        died\n",
       "1021    survived\n",
       "1022        died\n",
       "1023        died\n",
       "1024        died\n",
       "1025        died\n",
       "1026        died\n",
       "1027    survived\n",
       "1028        died\n",
       "1029        died\n",
       "1030        died\n",
       "1031    survived\n",
       "1032        died\n",
       "1033        died\n",
       "1034        died\n",
       "1035        died\n",
       "1036        died\n",
       "1037    survived\n",
       "1038        died\n",
       "1039        died\n",
       "1040        died\n",
       "1041        died\n",
       "1042        died\n",
       "Name: survived, Length: 1043, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['pclass' , 'sex' , 'embarked' , 'survived']]\n",
    "sur = []\n",
    "for i in data.survived.tolist():\n",
    "    if i == '1.0':\n",
    "        sur.append('survived')\n",
    "    else:\n",
    "        sur.append('died')\n",
    "        \n",
    "data.survived =  sur\n",
    "\n",
    "for col in data.columns.tolist():\n",
    "    data[col] = data[col].astype(str)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
